{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unstructured mesh done properly with Scala + GraphX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll do the unstructured mesh with graphx in scala, the native language of Spark.\n",
    "\n",
    "First we import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to define the mesh geometry; we'll define some constants about the mesh and distribute them to all worker processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val n = 11\n",
    "val npts = n*n\n",
    "val dx_val = 1./(n-1.)\n",
    "val dy_val = 1./(n-1)\n",
    "\n",
    "val mesh = sc.broadcast((n, npts, dx_val, dy_val))\n",
    "\n",
    "val npts = mesh.value._2\n",
    "val nx = mesh.value._1\n",
    "val ny = mesh.value._1\n",
    "val dx = mesh.value._3\n",
    "val dy = mesh.value._4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to create the grid geometry follow; we'll use a triangular mesh which in this case is regular but\n",
    "we're not asusming that anywhere in the calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type nodetype = (Double, (Double, Double))  // (dens, (x, y))\n",
    "type edgetype = (Double, Double)            // (deltax, deltay)\n",
    "\n",
    "val idx_to_grid : Long => (Long, Long) = idx => (idx % nx, idx/nx)\n",
    "val grid_to_idx = (i:Long, j:Long) => j*nx+i\n",
    "val valid_point = (i:Long, j:Long) => i >= 0 && i < nx && j >= 0 && j < ny\n",
    "val idx_to_x = (idx:Long) => idx_to_grid(idx)._1*dx\n",
    "val idx_to_y = (idx:Long) => idx_to_grid(idx)._2*dy\n",
    "val deltax = (start:Long, end:Long) => idx_to_x(end) - idx_to_x(start)\n",
    "val deltay = (start:Long, end:Long) => idx_to_y(end) - idx_to_y(start)\n",
    "   \n",
    "def edges_from_idx( idx:Long ): List[Edge[edgetype]] = {\n",
    "    val (i, j) = idx_to_grid( idx )\n",
    "    val deltas : List[(Long, Long)] = List((+1,0), (0,+1), (+1,+1))\n",
    "    return deltas.map(d => (i+d._1, j+d._2)).\n",
    "                  filter(ij => valid_point(ij._1, ij._2)).\n",
    "                  map(ij => grid_to_idx(ij._1, ij._2)).\n",
    "                  map(end => Edge(idx, end, (deltax(idx, end), deltay(idx, end))))\n",
    "}\n",
    "\n",
    "def node_from_idx( idx:Long ) : (VertexId, nodetype) = {\n",
    "    val initial_posx = 0.3\n",
    "    val initial_posy = 0.3\n",
    "    val sigma = 0.15\n",
    "    \n",
    "    val x = idx_to_x(idx)\n",
    "    val y = idx_to_y(idx)\n",
    "    val distx = x - initial_posx\n",
    "    val disty = y - initial_posy\n",
    "    val density = scala.math.exp(-(distx*distx + disty*disty)/(sigma*sigma))\n",
    "    \n",
    "    return (idx, (density, (x, y)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct the graph from the list of edges and nodes.  The nodes contain their density value and also their\n",
    "position, simply for easy of plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val nodelist = (0L to npts-1).toList.map(node_from_idx)\n",
    "val vertices: RDD[(VertexId, (Double, (Double, Double)))] = sc.parallelize(nodelist) \n",
    "\n",
    "val edgelist = (0L to npts-1).toList.map(edges_from_idx).flatten\n",
    "val edges: RDD[Edge[(Double, Double)]] = sc.parallelize(edgelist)\n",
    "\n",
    "val graph = Graph(vertices, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the graph we'll save it as a text file, read it in in python which has nice plotting capabilities, and plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_graph(name:String, graph:Graph[nodetype, edgetype]) : Unit = {\n",
    "    graph.edges.map(e => e.srcId + \",\" + e.dstId).saveAsTextFile(name+\"-edges.txt\")\n",
    "    graph.vertices.map(v => v._1 + \",\" + v._2._1 + \",\" + v._2._2._1 + \",\" + v._2._2._2).saveAsTextFile(name+\"-vtxs.txt\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_graph(\"init\", graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/usr/lib/pymodules/python2.7/matplotlib/lines.py:503: RuntimeWarning: invalid value encountered in greater_equal\n",
       "  return np.alltrue(x[1:] - x[0:-1] >= 0)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%PySpark\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def load_graph(name):\n",
    "    edgeRDD = sc.textFile(name+'-edges.txt').collect()\n",
    "    edges = [tuple(int(item) for item in tuple(edge.split(','))) for edge in edgeRDD]\n",
    "    \n",
    "    vertexRDD = sc.textFile(name+'-vtxs.txt').collect()\n",
    "    vertexdata = [tuple(float(item) for item in tuple(line.split(','))) \n",
    "                  for line in vertexRDD]\n",
    "                  \n",
    "    xydict = {int(id):(x,y) for id, dens, x, y in vertexdata}\n",
    "    denss = [dens for id, dens, x, y in vertexdata]\n",
    "    xs = [x for id, dens, x, y in vertexdata]\n",
    "    ys = [y for id, dens, x, y in vertexdata]\n",
    "    \n",
    "    lines = [(xydict[s], xydict[e]) for s, e in edges]\n",
    "    return xs, ys, denss, lines\n",
    "\n",
    "def show_results(name):\n",
    "    x, y, dens, lines = load_graph(name)\n",
    "    plt.tricontourf(x, y, dens, cmap=plt.cm.Blues)\n",
    "    plt.plot(x, y, 'ko', markersize=1)\n",
    "    \n",
    "    xlines = [x for line in lines for x in [line[0][0], line[1][0], None]]\n",
    "    ylines = [y for line in lines for y in [line[0][1], line[1][1], None]]\n",
    "\n",
    "    plt.plot(xlines, ylines, 'k-', linewidth=.5, alpha=0.25)\n",
    "    plt.savefig(name+'.png')\n",
    "    plt.close()\n",
    "\n",
    "show_results('init')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Initial](init.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we define the advection.  We'll set up a velocity and a timestep, and then create the advection \n",
    "routine: we collect all the terms necessary at each vertex and do the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val u = sc.broadcast(Array(1., 1.))\n",
    "val dt = sc.broadcast(0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type msgtype = (Double, (Double, Double), Double, Double, Double, Double, Double)\n",
    "\n",
    "def src_msg(dxy:(Double, Double), srcAttr: nodetype, dstAttr: nodetype) : msgtype = {\n",
    "   val (dx, dy) = dxy \n",
    "   val (srcDens, (xs, ys)) = srcAttr\n",
    "   val (dstDens, (xd, yd)) = dstAttr\n",
    "   return (srcDens, (xs, ys), dx*dx, dy*dy, dx*dy, (dstDens-srcDens)*dx, (dstDens-srcDens)*dy)\n",
    "}\n",
    "\n",
    "def dest_msg(dxy:(Double, Double), srcAttr: nodetype, destAttr: nodetype) : msgtype = {\n",
    "   val (dx, dy) = (-dxy._1, -dxy._2)\n",
    "   return src_msg((dx, dy), destAttr, srcAttr)\n",
    "}\n",
    "\n",
    "def apply_update(id:VertexId, attr:msgtype) : nodetype = {\n",
    "    val (dens_0, point, dx2, dy2, dxdy, drhodx, drhody) = attr\n",
    "    val weighted_dy_dx = dxdy/dx2\n",
    "    val grad_y = (drhody - weighted_dy_dx * drhodx) / ( dy2 - weighted_dy_dx * dxdy + 0.001)\n",
    "    val grad_x = (drhodx - dxdy*grad_y)/(dx2 + 0.001)\n",
    "    return ((dens_0 - dt.value*(grad_x*u.value(0) + grad_y*u.value(1))), point)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def step(g:Graph[nodetype, edgetype]) : Graph[nodetype, edgetype] = {\n",
    "    val terms = g.aggregateMessages[msgtype](\n",
    "        // Map\n",
    "        triplet => { \n",
    "            triplet.sendToSrc(src_msg(triplet.attr, triplet.srcAttr, triplet.dstAttr))\n",
    "            triplet.sendToDst(dest_msg(triplet.attr, triplet.srcAttr, triplet.dstAttr))\n",
    "          },\n",
    "        // Reduce\n",
    "        (a, b) => (a._1, a._2, a._3 + b._3, a._4 + b._4, a._5 + b._5, a._6 + b._6, a._7 + b._7)\n",
    "    )\n",
    "    \n",
    "    val new_nodes = terms.mapValues((id, attr) => apply_update(id, attr))\n",
    "    \n",
    "    return Graph(new_nodes, graph.edges)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All done! We now have a fault-tolerant unstructured mesh advection program.  We can run 200 steps and see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var i = 0\n",
    "var newgraph = graph\n",
    "for (i <- 1 to 200) {\n",
    "    newgraph = step(graph)\n",
    "}\n",
    "save_graph(\"final\", newgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%PySpark\n",
    "show_results(\"final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![one step](final.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
