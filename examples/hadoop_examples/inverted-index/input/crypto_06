Hang With Your Buddies to Resist Intersection Attacks
UNPUBLISHED PREPRINT

David Isaac Wolinsky, Ewa Syta, and Bryan Ford
Yale University

{david.wolinsky,ewa.syta,bryan.ford}@yale.edu

arXiv:1305.5236v1 [cs.CR] 22 May 2013

ABSTRACT
Some anonymity schemes, such as DC-nets and MIX cascades, can guarantee anonymity even against traffic analysis-- provided messages are independent and unlinkable. Users in practice often desire pseudonymity--sending messages intentionally linkable to each other but not to the sender--but pseudonymity in dynamic networks exposes users to intersection attacks. We present Buddies, the first systematic attempt to offer intersection attack resistant pseudonyms in practical anonymity systems. Buddies groups users dynamically into buddy sets, controlling message transmission to make buddies within a set behaviorally indistinguishable to a traffic-monitoring adversary. Intersection attack resistance does not come "for free," of course, and Buddies offers users control over the inevitable tradeoffs between anonymity, latency, and the useful lifetime of a pseudonym. Using tracebased simulations and a working prototype, we find that Buddies can guarantee non-trivial anonymity set sizes in realistic chat/microblogging scenarios, for both short-lived and long-lived pseudonyms.

1.

INTRODUCTION

Anonymous group communication systems, such as DCnets [3, 32, 4] and MIX cascades [24, 1], offer traffic analysis resistance even against powerful adversaries--provided all messages are independent of each other and/or the set of participants never changes. Realistic systems exhibit churn in the set of users online at a given time, however, and users often wish to send messages intentionally linkable to each other or to a common pseudonym. By sending linkable messages in the presence of this churn, however, users can quickly lose anonymity to statistical disclosure or intersection attacks [25, 15, 6]. Suppose Alice writes a blog reporting on corruption in her local city government. To protect herself, she always connects via Tor [8] to the server hosting her blog, and never reveals any personally identifying information on her blog or to the server hosting it. Carol, a corrupt local official targeted in Alice's blog, deduces from the blog's content that its owner is local, and calls her friend Mallory, a network administrator in the monopolistic local ISP. Mallory cannot directly compromise Tor, but he can read from Alice's blog the times and dates each of her 57 blog entries were posted, and he can analyze the ISP's access logs to see which customers were online at each of those times. While there were thousands of customers online at each posting time, Mallory finds that each customer--except for Alice--was offline during some posting event. The intersection of the 57 relevant online user sets thus completely de-anonymizes Alice.

As a step toward addressing such risks we present Buddies, the first anonymous communication architecture we are aware of designed to protect users systematically from network-monitoring adversaries capable of long-term intersection attacks. Buddies works by continuously maintaining an anonymized user database of participating users and their online status, and uses this information to simulate intersection attacks that a network-monitoring adversary might perform. These simulations yield two relevant anonymity metrics that Buddies reports continuously, as an indication of potential vulnerability to intersection attack: a possibilistic metric roughly measuring "plausible deniability," and a more conservative indistinguishability metric indicating vulnerability to more powerful statistical disclosure attacks [6]. Beyond measuring potential vulnerability, as prior work in metrics [7, 26] and alternate forms of anonymity [14] have done, Buddies attempts to offer active control over anonymity loss under intersection attack. Users set per-pseudonym policies to balance attack protection against communication responsiveness and availability. Active control depends on a policy module that monitors and filter the set of users active in each round, forcing the system to behave as if some online users were actually offline. Mitigation policies can enforce lower bounds on anonymity metrics, preventing Alice from revealing herself to Mallory by posting at the wrong time for example. Policies can also reduce the rate of anonymity loss to intersection attacks, for example by tolerating anonymity set members who are normally reliable and continuously online but lose connectivity for brief periods. Finally, policies can adjust posting rates or periods, enabling Buddies to aggregate all users coming online within a posting period into larger anonymity sets. If Alice sets her blog's posting period to once per day, for example, then Buddies can maintain Alice's anonymity among all users who "check in" at least once a day--any time during each day--even if many users check in only briefly at varying times. Buddies' architecture may be treated as an extension to various existing anonymous communication schemes, but is most well-suited to schemes already offering some measurable protection guarantees against traffic analyis, such as MIX cascades [24, 1], DC-nets [3, 29, 32], or verifiable shuffles [22, 11]. We have built a working prototype of Buddies atop Dissent [4, 32, 5], a recent anonymous communication system that combines verifiable shuffle and DC-net techniques. The prototype's design illustrates solutions to practical challenges such as to decentralize Buddies' design and distributed trust, to create and manage pseudonyms while maintaining their independence, and to allow users to attach different policies to each pseudonym.

ious operational decisions, the Anonymizer consults a Policy Oracle. By design the Policy Oracle has no access to sensitive information: it makes decisions based purely on public information available to anyone. We assume the network-monitoring adversary identifies users by some kind of network identifier or locator (e.g., IP address). The adversary can tell which users (or network locators) are online or offline at any given moment, and how much data they transmit or receive, but cannot see the actual content of data communicated between the (honest) users and the Anonymizer. These assumptions model an ISP-grade adversary that can implement "wholesale" networklevel monitoring of users connected via that ISP.

2.1 Overview of Operation
In Buddies' conceptual architecture, communication proceeds synchronously through a series of rounds. The Anonymizer To evaluate Buddies' practicality in realistic online comdrives the operation of each round i, as follows: munities, we analyze IRC trace data under a Buddies sim1. Registration: At the start of round i the Anonymizer ulator, exploring questions such as how effective Buddies' updates the membership roster, Mi , to include members who anonymity metrics are, how feasible it may be to maintain may have recently joined. nontrivial anonymity sets restistant to intersection attacks 2. Nym creation: The Anonymizer next creates and anfor extended periods, and how effectively Buddies can limit nounces one "fresh" Nym Ni each round. For each new Nym, loss of anonymity while preserving communication responthe Anonymizer chooses one User uniformly at random as siveness and availability. the Nym's owner, keeping this ownership secret. A Nym's This paper's primary contributions are: (a) the first anonymity lifetime is in principle unlimited: over time users acquire architecture designed to address intersection attacks systemfresh Nyms at random but "statistically fair" times. (We atically; (b) a modular, policy-based framework for both vullater address creation of larger "batches" of Nyms efficiently, nerability monitoring and active mitigation of anonymity so new users need not wait a long time before they can post.) loss via intersection attacks; (c) an evaluation of Buddies' 3. Scheduling: The Anonymizer consults the Policy Orpracticality via a working prototype and trace-based simuacle to choose one Nym, Ti , for transmission in this round, lations reflecting realistic online communities. from all Nyms in existence. The Policy Oracle also specifies Section 2 of this paper presents a simplified but illustrathe number of bits Bi that the owner of Nym Ti may post. tive high-level model of Buddies' architecture and operation, (Scheduling multiple Nyms per round is a straightforward and an informal analysis of intersection attacks and metextension.) As the Policy Oracle can access only public inrics applicable to this model. Section 3 then explores useful formation, scheduling cannot depend directly on which Users attack mitigation policies. Section 4 details challenges and currently "have messages to post." Scheduling can depend on approaches to integrating Buddies into practical anonymity other factors, however, such as Nyms' lifetimes, recent ussystems, and Section 5 experimentally evaluates both our age, or the interest of other users as indicated in anonymous working Buddies prototype and trace-based simulations, and "Approve" messages on other Nyms. Section 6 concludes. 4. Message submission: The Anonymizer announces the scheduled Nym Ti and transmission length Bi to the 2. BUDDIES ARCHITECTURE Users currently online. Each online user submits exactly Bi secret bits to the Anonymizer. The bits sent from any user Figure 1 shows a high-level conceptual model of the Budj other than the owner of Nym Ti represent "cover traffic" dies architecture. Buddies assumes there is some set of users, necessary to hide the Nym-owner's message submission from each of whom has a secret (i.e., securely encrypted) network traffic analysis. The Anonymizer forms an online user set, communication path to a component we call the Anonymizer. Oi  Mi , consisting of the users who submitted a (real or For now we conceptually treat the Anonymizer as a central, cover) message in round i. trusted "black box," although later we will map this con5. User filtering: The Anonymizer now consults with ceptually centralized component to realistic anonymization the Policy Oracle, giving the Policy Oracle the set Oi of systems that decentralized trust, to avoid trusting any single online users (but not any information about which, if any, physical component or administrative domain. of these users owns the Nym scheduled this round). The Buddies' basic model is inspired by anonymous blogging Policy Oracle returns a new, filtered user set Pi  Oi , further or IRC scenarios, where anonymous communication is "oneconstraining the set of online users whose submissions the way" from users to a public forum, and users primarily desire Anonymizer will actually accept this round. sender anonymity [23]. While we expect Buddies to general6. Message posting: If the owner of the scheduled Nym ize to two-way models and metrics [27], we defer such extenTi is a member of Pi --i.e., is online and was not filtered sions to future work. Each Buddies user "owns" some number above--then the Anonymizer decrypts that user's secret mesof Nyms, each representing a pseudonymous identity under sage and posts it in association with Nym Ti . If the owner which the owner may post: e.g., an anonymous chat handle of Ti is not in Pi --either because the owner was not online or blog. Users may secretly submit messages to be posted to or was filtered above--then the Anonymizer posts Bi zero Nyms they own, which the Anonymizer scrubs of identifying bits to Nym Ti . information and publicly "posts" to that Nym. To make varFigure 1: Conceptual model of Buddies architecture

2.2 Active Mitigation of Intersection Attacks
The user filtering step above (step 5) serves as Buddies' primary "control point" through which to resist intersection attacks. The Policy Oracle uses publicly available information to simulate a virtual Adversary, by continuously performing an "intersection attack" against each Nym. At step 5 of each round i, the Policy Oracle first forms an attack model for the scheduled Nym Ti , based on prior history and the set Oi of users online in this round. The Policy Oracle computes one or more relevant anonymity metrics as detailed further below, and determines if action is required to limit or avoid anonymity loss in this round. If no action is required, the Policy Oracle returns the unfiltered user set to the Anonymizer, i.e., Pi = Oi . If action is required, however, then the Policy Oracle can filter the user set producing a Pi  Oi , thus preventing any user not in Pi from posting, as if more users were offline than are actually offline. To illustrate how this filtering enables the Policy Oracle to mitigate intersection attacks, consider the following strawman policy. In step 5 of each round, the Policy Oracle simply checks whether all known users are presently online, i.e., whether Oi = Mi , returning Pi = Mi if so, and otherwise returning Pi = . In effect, the Policy Oracle forbids the system from making progress--allowing anyone to post to any Nym--except when all users are online. Since messages are posted only when all users are online, the intersection of all nonempty rounds' user sets is Mi , and the system preserves "perfect" anonymity assuming the Anonymizer performs as required. The tradeoff, of course, is effective system availability, which would be unusable in most practical situations. The key technical challenge, and a primary contribution of this paper, is developing more nuanced methods of controlling the user filtering step in each round. By controlling these filtering choices, we seek to maintain both measurable anonymity levels under intersection attack and "usable" levels of availability, under arguably realistic conditions. There are of course situations in which no active control mechanism will help. If all users except one go offline permanently at one point, for example, the Policy Oracle ultimately has only two choices: eventually allow the one remaining user to post, giving up all anonymity under intersection attack; or filter that user forever, giving up availability completely. Thus, we need to set realistic expectations, and a secondary contribution of this paper is the use of IRC trace datasets in Section 5 to develop methods of evaluating the feasibility of resisting intersection attacks in realistic online user communities, and testing realistic control policies against these feasibility metrics. Buddies' architecture separates the Policy Oracle from the Anonymizer, giving the Policy Oracle access only to "public information" we assume is known to everyone including the adversary, eliminating the risk that policies may "accidentally" compromise anonymity by leaking ownership information. By architecturally disallowing the Policy Oracle from having access to private information, we avoid the need to analyze each policy carefully for such "side-channel" anonymity leaks, and instead can focus purely on the main question of how effectively a policy mitigates intersection attacks while maintaining usable levels of availability. Another important question is whether all the information the Policy Oracle needs to simulate the Adversary's intersection attacks--such as the set of users online in each round-- should be considered "public information." Even if we assume

that a strong global adversary would know this information anyway, most realistic adversaries will be unable to monitor all users. If Buddies' design "hands out" information that would otherwise be at least partially private--such as the IP addresses of all online users--we risk accidentally "strengthening" a weak adversary into an effectively omniscient adversary. In the important case of users' network identities such as IP addresses, our practical Anonymizer designs mitigate this leak by replacing IP addresses with anonymous tags when reporting online user sets to the Policy Oracle, as discussed later in Section 4.3. However, whether Buddies' simulation-based architecture may strengthen otherwise-weak adversaries in other unexpected ways, by making "too much information" public, is a question requiring further analysis.

2.3 Analyzing Intersection Attacks
While we do not attempt full formal analysis, the simplicity of the conceptual model facilitates straightforward informal analysis. Our focus here is on a particular class of attacks, namely what an adversary can learn from users' online status over time (the "switches" in Figure 1). The many other known attacks against practical anonymity systems are important but out of this paper's scope. We also claim no particular novelty in our analysis techniques or metrics; our goal is merely to apply known attacks [25, 15, 6] and anonymity metrics [7, 26] to the Buddies model. We assume the Anonymizer is trusted to "do its job," keeping secret the linkage between Users and the Nyms they own. We also assume honest Users--those users we care about protecting--do not "give away" their identities or the relationships between their Nyms via the messages they post. Under these conditions, the Adversary obtains three potentially important pieces of information in each round i: (a) the set of online users Oi , (b) the set Pi of online users who passed the Policy Oracle's filter in step 5, and (c) the Bi message bits that were posted to the scheduled Nym Ti . An observation that will be key to Buddies's design is that only (b) and (c) will actually prove relevant to intersection attack analysis: the adversary ultimately obtains no useful information from knowing which users were online during a given round, beyond what the adversary learns from knowing which users were online and unfiltered. Since we assume honest users do not "give away" their identities in their message content, we ultimately care only whether the message posted to Nym Ti was zero or nonzero. If a nonzero message appeared for Nym Ti in round i, then the adversary infers that the owner of Ti must be a member of the filtered user set Pi in that round. (If the owner of Ti was online but filtered in round i, then a zero message would have appeared, as if the owner was offline.) On the other hand, if a zero message appears for Ti in round i, then there are three possibilities: (1) the owner of Nym Ti was offline, (2) the owner was online but filtered, or (3) the owner was online and unfiltered, but chose to post a zero message (e.g., because the owner had no message to send).

2.3.1 Possibilistic Anonymity Analysis
To construct a simple possibilistic anonymity set PN for a given Nym N , the adversary intersects the filtered user sets Pi across all rounds i for which Nym N was scheduled and a nonzero message appeared: i.e., PN = i {Oi | Ti = N  mi = 0}. Thus, PN represents the set of users that might conceivably own Nym N , consistent with the observed set

of nonzero messages that have appeared for Nym N up to any point in time. We define the size of a Nym's possibilistic anonymity set, |PN |, as Nym N 's possibilistic anonymity, which for convenience we abbreviate as possinymity. Although possinymity is only one of the many useful anonymity metrics that have been proposed [16, 17], and perhaps a simplistic one, we feel it captures a useful measure of "plausible deniability." If for example a user is dragged into court, and the judge is shown network traces of a Buddies system in which the accused is one of |PN | users who may in principle have posted an offending message, then a large possibilistic anonymity may help sow uncertainty of the user's guilt. We fully acknowledge the weaknesses of plausible deniability in general, however, especially in environments where "innocent until proven guilty" is not the operative principle.

2.3.2 Probabilistic Anonymity Analysis
While a simplistic adversary might stop at the above analysis, a smarter adversary can use probabilistic reasoning to learn not only from rounds in which messages appeared but also from rounds in which no message appeared. For example, suppose the adversary correctly guesses that, in each round i, the owner of Nym N will have no useful (nonzero) message to post with some independent and uniformly random probability p--i.e., will "pass"--and with probability 1 - p will have some nonzero message to post. For simplicity assume there are only two users A and B , the adversary observes exactly one round i in which Nym N was scheduled but the posted message mi = 0, and Pi = {A}, i.e., A participated but B was excluded. No message appears in round i if either of two events occur: (a) A owns N , but chose with probability p not to post in round i; or (b) B owns N , hence no message appeared independent of p because B did not participate in round i. Because Nyms are assigned to users uniformly at random on creation, the "base" probability that either user owns N is 1/2. The probability of the above events (a) and (b) occurring conditioned on the observed history, however, is different. To be precise, P [(a) | (a)  (b)] = P [(a)	((a)  (b))]/P [(a)  (b)] = (p/2)/P [(a)  (b)], and P [(b) | (a)  (b)] == (1/2)/P [(a)  (b)]. Since (a) and (b) are disjoint events, P [(a)  (b)] = P [(a)] + P [(b)], so P [(a)] = (p/2)/(p/2 + 1/2) = p/(p + 1), and P [(b)] = (1/2)/(p/2 + 1/2) = 1/(p + 1). From the adversary's perspective, observing one round in which no message appears for Nym N , and in which A participated but B did not, reduces the relative likelihood of A being the owner by a factor of p. Observing similar events across multiple rounds exponentially increases the adversary's "certainty" of B being the owner: after k such rounds, the likelihood of A being the owner is only pk /(pk + 1).

Thus, instead of relying on the dubious relevance of any particular probabilistic analysis--which may break the moment anyone slightly refines an existing attack--Buddies' resistance to probabilistic attacks relies on an indistinguishability principle that applies to all attacks of this form independent of specific probabilities involved. In particular, if two users A and B have exhibited identical histories with respect to inclusion or exclusion into each round's filtered user set Pi , across all rounds i in which a given Nym N was scheduled, then under any probabilistic analysis of the above form the adversary must assign identical probabilities to A and B owning Nym N . That is, if for every round i, it is the case that (A  Oi )  (B  Oi ), then users A and B are probabilistically indistinguishable from each other, hence equally likely to own Nym N . For any user A and Nym N , we define A's buddy set BN (A) as the set of users that are probabilistically indistinguishable from A, including A itself, with respect to potential ownership of Nym N . If n users are probabilistically indistinguishable from A, then under the attacker's analysis each such user in BN (A) has an individual probability no greater than 1/|BN (A)| of being the owner of N . Intuitively, buddy-sets form equivalence classes of users who "hang together" under probabilistic intersection attacks, so that individual buddies do not "hang separately." We next define a second anonymity metric, indistinguishability set size, or indinymity for short, as the size of the smallest buddy-set for a given Nym N . Since we do not know how a real attacker will actually assign probabilities to users, indinymity represents the minimum level of anonymity a member of any buddy set can expect to retain, even if the adversary correctly intersects the owner's anonymity set down to the members of that buddy set. Thus, the attacker cannot (correctly) assign a probability greater than 1/|BN | to any user--including, but not limited to, the owner of N . One might argue that we "mainly" care about the buddy set containing the true owner of N , not about other buddy sets not containing the owner. A counter-argument, however, is that a particular observation history might make some other buddy set falsely appear to the adversary are "more likey" to own N . In this case, we may well care how much protection the "innocent" members of that "unlucky" buddy set have against being "falsely accused" of owning N . Thus, to ensure that all users have the "strength in numbers" of being indistinguishable in a crowd of at least n users, regardless of how the adversary assigns probabilities, we must ensure that all buddy sets have size at least n.

3. ATTACK MITIGATION POLICIES
Based on the above architecture and attack analysis framework, we now explore some more realistic policies by which the Policy Oracle might mitigate loss of anonymity due to intersection attacks. We make no claim that these are the "right" policies--they merely represent a starting point for ongoing refinement. Two key benefits of Buddies' architecture, however, is to modularize these policies into replaceable components independent of the rest of the anonymous communication system so they can be further evolved easily, and to ensure by system construction that policies cannot leak anonymity information other than by failing to protect adequately against intersection attacks. We first explore the goal of maintaining possinymity-- large possibilistic anonymity sets--then the goal of guar-

2.3.3 Indistinguishability Under Probabilistic Attack
The above reasoning generalizes to many users, to varying probabilities of posting, etc. The focus of this paper is not on refining or deepening such analysis, however, a goal admirably addressed by relevant prior work [7, 26]. Instead, our goal is to achieve a measurable level of resistance to unknown probabilistic intersection attacks: we do not know the probabilities with which users will or won't choose to post in particular rounds, how well or poorly the (unknown) attacker may be able to predict when the owner of a given Nym will or won't post, etc.

anteeing a minimum worst-case level of indinymity--indistinguishability under probabilistic attack. An important caveat with any anonymity metric, however, is that Buddies cannot guarantee that measured anonymity necessarily represents good anonymity, if for example an attacker can compromise many users or create many Sybil identities [9]. Appendix A discusses these issues in more detail.

offline before being eliminated--as the Nym's possinymity set size approaches the user's specified lower bound. Such a policy might in essence trade more temporarily unavailability for greater total Nym longevity. Users Worth Waiting For. While the above simple variants suggest starting points, we envision many ways to refine policies further, for example by recognizing that a user's record of past behavior is often a predictor of future behavior. To maximize a Nym N 's possinymity and minimize loss rate, we may wish to consider some members of N 's current possinymity set "more valuable" than others, in that they have remained online and participating reliably for a long period with at most a few brief offline periods, and hopefully will continue to exhibit similar reliability. In particular, a policy might apply an offline-time threshold as discussed above to limit loss rate, but apply different thresholds to different members of N 's current possinymity set: e.g., longer, more generous thresholds to more "valued" users. The Policy Oracle can build up reliability information about users starting when those users first appear--not just when a particular pseudonym of interest is created. Thus, a policy that Buddies applies to a particular Nym N can benefit from user history state that the Policy Oracle built up before N was created.

3.1 Maximizing Possinymity
The possinymity metric defined in Section 2.3.1 considers only rounds in which nonzero messages appear for some Nym N , intersecting the filtered user sets across all such rounds to determine N 's possibilistic anonymity set PN . We consider several relevant goals: maintaining a given possinymity threshold, mitigating the rate of possinymity loss, or both. Maintaining a Possinymity Threshold. Suppose a dissident, posting anonymously in a public chat room under a Nym N , wishes to maintain "plausible deniability" by ensuring that |PN |  100 throughout the conversation--and would rather be abruptly disconnected from the conversation (or have Nym N effectively "squelched") than risk |PN | going below this threshold. As a straightforward policy for this case, at step 5 of each round i, the Policy Oracle computes the new possinymity that N would have if Oi is intersected with N 's "running" possinimity set from the prior round, and return Pi = Oi if the new possinymity remains above threshold, or Pi =  otherwise. In practice, the effect is that N 's possinymity starts out at an initial maximum of the total set of users online when the user first posts via N , then decreases down to (but not below) the possinymity threshold as other users go offline, either temporarily or permanently. This policy has the advantage of not reducing the usability of N , or artificially delaying the time at which the user's posts appear, as long as the possinymity set remains above-threshold. However, once N 's possinymity set reaches the set threshold, all of the users remaining in this set become critical, in that N becomes unusable for posting once any remaining member goes offline. In the "dissident scenario" we envision this event might be the user's signal to move to a new network location: e.g., get a fresh IP address at a different Internet cafe. Limiting Possinymity Loss Rate. An alternative, or complementary, goal is to reduce the rate at which N 's possinymity decreases. In realistic scenarios, as our trace data in Section 5 illustrates, clients often get delayed or disconnected temporarily but return soon thereafter. Thus, a more refined policy might temporarily halt all posting for Nym N --by returning Pi =  when members of N 's current possinymity set go offline, in hopes that the missing members will soon return. To get N "unstuck" if members remain offline for a threshold number of rounds, however, then the policy eliminates these persistently offline members from N 's permanent possinymity set by returning a smaller (but nonempty) Pi . Such a policy may "filter out" possinymity set losses due to otherwise-reliable users going offline briefly, at the potential cost of delaying a user's posting to Nym N for a few rounds if some current possinymity set member goes offline permanently. Of course, a loss rate limiting policy may readily be combined with a thresholdmaintaining policy of the form above. A further refinement of this combination might be to increase the loss limiting policy's "tolerance"--number of rounds a user may remain

3.2 Guaranteeing Minimum Indinymity
The possinymity metric of interest above considers intersection attacks only across rounds in which messages appeared for some Nym N , but in realistic anonymous chat or blogging scenarios a user's posts are likely to be interspersed with periods in which no message appears, and a smarter adversary can use predictive techniques as discussed in Section 2.3.2 to glean probabilistic information from rounds in which no message appeared. We therefore wish to offer policies that can guarantee users some level of indinymity, or indistinguishability even under probabilistic attack. Forming and Enforcing Buddy-Sets. To guarantee a Nym N will have a minimum indinymity of some value K , the Policy Oracle must ensure that all of N 's buddy-sets--subsets of users whose members exhibit identical online behavior across rounds after user filtering--are all of size at least K . As a straw-man approach to buddy-set formation, on the creation of a Nym N , the Policy Oracle could simply divide N 's initial user-set O into |O|/K  arbitrary buddy-sets "up-front," each containing at least K users. At step 5 of each round i, for each buddy-set B containing any offline user u  Oi , the Policy Oracle removes all members of u's buddy set B from the filtered user set Pi it computes. Thus, the Policy Oracle effectively forces members of each buddy set to come online or go offline "in unison," ensuring that they remain permanently indistinguishable under probabilistic intersection attack (even if the adversary can distinguish one buddy set from another). This straw-man policy is likely to yield poor availability, however, as it prevents N 's owner from posting whenever any member of the owner's buddy-set is offline, making N unusable in the fairly likely event that any member of this buddy set is unreliable or disappears permanently. Lazy Buddy-Set Formation. As a first refinement, therefore, the Policy Oracle can delay buddy-set decisions until users actually go offline. At cre-

ation, a Nym N starts with one large buddy-set consisting of its entire initial user set O. The first round i in which member(s) of O go offline, the Policy Oracle might first delay all posting for Nym N by returning Pi = , in hopes the missing member(s) will return online soon, as discussed above. Once the Policy Oracle decides to "give up" waiting for one or more members, however, it splits N 's current buddy-sets into two, isolating all the offline members into one of the resulting buddy-sets. Thus, by delaying the decision of how to split buddy-sets until such a decision is critically needed, the Policy Oracle guarantees that after the split-point, the buddy-set containing only online members will have a chance to "make progress"--at least until more users go offline for sufficiently long to force another buddy set split. After a split, each of of the resulting buddy-sets must be at least of size K to maintain an indinymity lower bound. If fewer than K users are actually offline (|O - Oi | < K ), then the Policy Oracle must "sacrifice" a few online users, placing them in the offline users' buddy-set. Otherwise, if for example a single offline user forcing a split is actually the owner of Nym N then placing the offline user in a buddy set of size 1 would quickly reveal to a probabilistic attacker that the now-offline user owned the pseudonym, once the attacker notices that posts have stopped appearing on Nym N . By "sacrificing" K - 1 additional users at the split point, even if the attacker infers from the absence of posts that N 's owner is in the now-offline buddy-set, he cannot tell whether the owner is the user who caused the split or those scarificed and forced offline to "keep him company." If the offline users in a buddy set eventually return online, then the whole buddy set rejoins in unision, making the Nym usable again if the owner was a member of this buddy set. Whom to Sacrifice. When the Policy Oracle splits a buddy set and must "sacrifice" online users to pad the offline buddy set to size K , an important question is how to choose which online users to sacrifice. We implemented and investigated two classes of sacrificial policies: random, and least reliable users. Random choice effectively clusters users into buddy sets without regard to reliability, which is simple but risks sacrificing highly reliable users by mixing them into buddy sets containing unreliable or permanently offline users. Alternatively, the Policy Oracle can first sacrifice the users with the weakest historical "reputation" for reliability: e.g., users who have not been online long and/or have exhibited long offline periods. In this way, we retain the users who have been most reliable in the past--and thus hopefully will continue to be most reliable in the future--in the buddy set partition that remains online immediately (and may split further if more nodes go offline in the future). We expect reliability-sensitive policies to maximize a Nym's effective lifetime, provided the Nym's true owner is one of the more reliable users and does not get sacrificed into an unreliable or permanently offline buddy set. A short-lived or unreliable user cannot expect his Nyms to be long-lived in any case, since a long-lived Nym must have a "base" of reliable, long-lived users to retain anonymity under intersection attack, and the Nym's owner must obviously be a member of the long-lived anonymity set it wishes to "hide in."

bility and perhaps unrealistically require all users to "agree on" one policy. Instead, Buddies allows each Nym to have a separate intersection attack mitigation policy--e.g., different possinymity and indinymity lower bounds--chosen by the Nym's owner. Since intersection attacks are by definition not an issue until a Nym N has been in use for more than one round, N 's owner specifies the policy parameters for a Nym N in its first post to N . The set of users online in this first message round, in which N 's policy is set, forms N 's initial anonymity set O, which is also the maximum anonymity set N can ever achieve under intersection attack. In subsequent rounds in which Nym N is scheduled, the announced policy for N determines the Policy Oracle's behavior in filtering N 's user sets to mitigate intersection attacks. Hence each Nym's policy is independent of that of other Nyms--including other Nyms owned by the same user. This policy independence, and the correctness of Section 2's analysis, depend on the fact that Nyms are assigned to users uniformly at random and independent of all other Nyms (Section 2.1). Otherwise, the choices the Policy Oracle makes on behalf of one Nym might well leak information about other Nyms. This leads to some specific design challenges addressed below in Section 4.2.

4. BUDDIES IN REAL SYSTEMS
Since Buddies' conceptual model in Section 2 is unrealistically simple in many ways, we now address key challenges of implementing Buddies in practical anonymity systems.

4.1 Decentralizing the Anonymizer
So far we have treated the Anonymizer as a trusted "black box" component, but in a practical anonymous communication system we do not want any single component to be trusted. We therefore replace Buddies' Anonymizer with one of the standard decentralized schemes for anonymous message transmission, such as mix-nets [2, 20] DC-nets [3, 31], or verifiable shuffles [22, 11]. These schemes essentially distribute trust across multiple nodes: e.g., by relaying users' messages through a series of mixes or shufflers, thereby (hopefully) protecting users' anonymity even if only one of these nodes is actually trustworthy and the rest are compromised or under the adversary's control. Many "ad hoc" mix-based and onion routing schemes are vulnerable to a variety of traffic analysis and active attacks, which Buddies does not attempt to address [18, 21, 28]. However, particular variants such as MIX cascades [24, 1] and verifiable shuffles [22, 11] offer formally provable anonymity guarantees even against limited traffic analysis (though not against intersection attacks when messages are linkable). These "strong" anonymity schemes are most suitable for Buddies, since without basic traffic analysis protection for unlinkable message traffic, we cannot expect to achieve traffic analysis protection for linkable posts via pseudonyms. DC-nets [3, 31] also offers the required security properties. Our Buddies prototype builds on Dissent [4, 32, 5], a recent anonymous communication system employing a combination of verifiable shuffle and DC-net techniques. Dissent's operation is based on synchronous communication rounds, in which clients representing users submit ciphertexts containing either encrypted messages or "cover traffic," on an agreed-upon transmission schedule; the cover traffic cancels with the encrypted message to reveal the anonymous plain-

3.3 Varying Policies and Nym Independence
So far we have assumed that the Policy Oracle enforces one "global policy" on all Nyms, but this would limit flexi-

text to all participants. For scalability and robustness to client churn, an anytrust group of Dissent servers supervises this process [32]; clients must trust that at least one server behaves honestly but need not know which server is honest. A further refinement makes submitted Dissent ciphertexts verifiable via cryptographic techniques [12, 5], reducing vulnerability to disruption, and enabling clients to "drop off" ciphertexts and participate even if connectivity is intermittent. Dissent's synchronous round-based structure, and its emphasis on offering anonymous group multicast suitable for chat or blogging scenarios, make it one (though not the only) appropriate foundation for Buddies' Anonymizer.

4.2 Creating Nyms and Assigning Ownership
Dissent periodically runs a secure but expensive verifiable shuffle, to set up a transmission schedule for a subsequent series of lower-latency DC-nets communication rounds. The shuffle creates a secret random permutation between participating users and transmission slots, such that each user knows only which slot is his own, and the Dissent servers do not know which slot any honest user owns. Since each user retains the same slot throughout each epoch until the next shuffle, anyone can tell which messages came from which users within an epoch--i.e., those that appeared in the same slot in different rounds--so slots essentially play a role of fixed-period "pseudonyms." However, slots alone fail to meet Buddies' requirements in three ways: (a) in Buddies we wish to allow Nyms an unlimited lifetime; (b) Buddies allows users to have multiple Nyms at once; and (c) a shuffle's 1to-1 mapping of users to slots violates Buddies' assumption that Nyms are assigned to users independently at random. The Nym lifetime issue is easily addressed by using public keys to represent long-lived Nyms across epochs. In the first message a client sends in a "fresh" transmission slot at the start of an epoch, the client either sends a new self-signed public key to create a new Nym, or else sends a message signed with a previous Nym's private key to link the transmission slot in this epoch to a previously existing Nym. Allowing multiple Nyms per user is also straightforward: we simply permit each user to submit a small fixed number k of secret input messages or keys to the verifiable shuffle, rather than just one per user. Each user obtains k independent transmission slots per epoch, each of which the user can use either to create a fresh Nym, or to extend a previous Nym's lifetime. A user can own more than k Nyms at once, but can only use up to k of them during a single epoch. The Nym Independence Problem. The final, more subtle issue is Buddies' assumption that Nyms are assigned to users independently at random. Since Dissent's shuffle creates a random 1-to-1 permutation, the assignment of transmission slots to users is not quite independent: with one slot per user (k = 1), for example, anyone including the adversary that if user A owns slot 1, then user A does not own slot 2. This lack of independence violates assumptions that Buddies' attack analysis makes in Section 2.3, and that different policies may be applied to each Nym (Section 3.3), creating risks of leaking anonymity. Suppose for example there are two users A and B , who introduce two nyms N1 and N2 in the first epoch. N1 announces a weak policy with a minimum buddy set size of 1, but N2 requires a buddy set size of 2. In some communication round the adversary observes a post to N1 while user B was offline, and correctly infers that A owns N1 .

In a pure Buddies system with fully independent Nym assignment, this inference gives the adversary no information about who owns N2 , since it is just as likely that A owns both N1 and N2 , as it is that each user owns one Nym. But if the system creates N1 and N2 via a 1-to-1 shuffle, A cannot own both N1 and N2 , enabling the adversary to leverage its inference about N1 to de-anonymize N2 as well. A straightforward, if inefficient, way to assign Nyms fully independently is to use a verifiable shuffle as a "lottery." We discard all but the first transmission slot in each shuffle's output permutation: only one lucky "winning user" per shuffle gets a slot, which he can use either to create a new Nym or to extend the lifetime of an existing one. We can still assign multiple (independent) slots per epoch by running multiple independent shuffles in parallel, at the cost of multiplying total shuffle cost by the number of slots created. As a tradeoff between practical efficiency and full independence, we might allow each of user to insert k  m ciphertexts into a shuffle, then discard all but the first 1/m of the resulting slots. Each user then obtains about k slots per shuffle in expectation, but some users will obtain more than others. Even after identifying k slots as belonging to user A, for example, the adversary cannot infer with certainty that A does not also own some other slot. Nevertheless, the resulting slot assignments are not fully independent, and a sophisticated adversary might still be able to make statistical inferences, especially across multiple epochs over which a long-lived Nym might be used. We leave detailed analysis of such tradeoffs, and exploration of better approaches to creating Nyms, as a topic for future work.

4.3 Implementing the Policy Oracle
We could implement the Policy Oracle as an independent server that the Anonymizer server(s) communicate with over the network, but doing so would require all clients to trust the Policy Oracle server to implement their requested attack mitigation policies correctly: although a bad Policy Oracle server cannot directly de-anonymize users since it does not know which users own each Nym, it could--by intent or negligence--simply make intersection attacks easy. For this reason, Buddies leverages the anytrust server model that the underlying Anonymizer already uses, running a virtual replica of the Policy Oracle in "lock-step" on each of the anonymization servers. The servers use standard distributed accountability techniques [13] to cross-check each others' computation of Policy Oracle decisions, halting progress and raising an alarm if any server deviates from an agreed-upon deterministic algorithm implementing the Policy Oracle. These techniques apply readily to the Policy Oracle precisely because it architecturally has access to no sensitive state, hence all its state may be safely replicated. Identifying Users to the Policy Oracle. Buddies' Anonymizer needs to communicate the set of users currently online in each round to the Policy Oracle, which implies that we must treat these online sets as "public information" that the adversary may also obtain. As discussed in Section 2.2, however, revealing actual user identities or locators for this purpose, such as users' public keys or IP addresses, risks strengthening a weak adversary into an "omniscient" adversary for intersection attack purposes. Buddies addresses this problem by permitting clients to authenticate to the servers via linkable ring signatures [19, 10]. To connect, each client generates a cryptographic proof

Percent of Members Posting

that it holds the private key corresponding to one of a ring of public keys, without revealing which such key the client holds. In addition, the client generates and proves the correctness of a linkage tag, which has a 1-to-1 relationship with the client's private key, but is cryptographically unlinkable to any of the public keys without knowledge of the corresponding private keys. The servers track which clients are online via their linkage tags, and provide only the list of online tags to the Policy Oracle in each round, so the Policy Oracle can simulate an adversary's intersection attacks without knowing which actual users are online each round. Of course, the server that a client connects to directly can associate the client's network-level IP address with its linkage tag, and a compromised server may share this information with an adversary. This linkage information does not help a global passive adversary, who by definition obtains all the same information the Policy Oracle obtains merely by monitoring the network, but may help weaker adversaries perform intersection attacks against those users who connect via compromised servers. We see this risk as equivalent to the risk clients run of connecting to a compromised "entry relay" in existing anonymity systems [20, 8]. Compromised servers are just one of the many avenues through which we must assume an adversary might monitor the network.

1 all data sets football iphone redhat stocks Members 0.1

0.01 0 2s 1min 1hr Max offline time 1day 1week

Figure 2: Maximum user offline time for various IRC rooms normalized for members.
0.5 0.1

0.01 all data sets football stocks iphone redhat 1 10 100 Posts 1000 10000

5.

EVALUATION

0.001

To evaluate Buddies' utility, we begin by investigating traces taken from popular public IRC (Internet Relay Chat) chat rooms on Efnet servers. We find these traces to be useful not only because IRC chat rooms are popular forums of the type Buddies is intended to support, but also because the IRC traces capture not only when users post messages, but also when they come online or go offline. We apply these traces to an event-based Buddies simulator. We consider na ive anonymous posting without Buddies, then posting under policies that enforce minimum buddyset sizes, and policies that attempt to maximize possinimty. Finally, we analyze the overheads Buddies induces in the context of Dissent, a real anonymity system.

0.0001

Figure 3: Number of posts per user across IRC rooms IRC recognizing the disconnection. This creates a period of time in which the user must use a secondary nickname, then switch back to the original nickname once IRC recognizes the disconnect. Unfortunately we have no statistics on average disconnect time, but we were able to identify these scenarios and "fix" the data such that the user appears to be continuously online. Our analysis begins with maximum offline time and number of posts by each member, shown in Figures 2 and 3. Our interest in maximum offline time as opposed to total online time reflects Buddies' need for users to be consistently online for extended periods of time, to maintain anonymity under intersection attacks. The number of users in each trace was: football 1207, iphone 2290, redhat 160, and stocks 558. In general, IRC data sets exhibit a common pattern in which a small set of dedicated users is almost always online, a larger setroughly 15% to 20%who show up about once an hour to once a day, and a large set of ephemeral users comprising upwards of 80% of the total population. We do not expect these ephemeral users to contribute usefully to the anonymity sets of long-lived pseudonyms, but it is important for Buddies to operate in their presence. Ephemeral users may also offer increased anonymity to short-lived pseudonyms, as we explore later. We first wish to set realistic expectations on anonymity achievable under intersection attack within a given online forum. For this purpose we first consider how many users we expect to be eligibile to contribute to the anonymity set of a long-lived pseudonym, in this case operating at a rate of one round per day. Figures 4 and 5 present the number

5.1 Simulator
We implemented an event-driven Buddies simulator using Python, cleverly called the Anonymity Simulator (AS). AS plays the role of users, the Anonymizer, and the Policy Oracle. As input the AS takes an IRC trace, time between rounds, system-wide buddy and possinymity set sizes, and buddy set formation policies. To establish reliability, AS can either use an initial period of the trace, bootstrap, to establish user online times or use a deity mode and review the entire data set to establish reliability times and begin executing the trace after the bootstrap period.

5.2 Datasets
The buddy system focuses on anonymous group communication systems which reveals all the output of the system to all members and minimally the input to all servers. This behavior maps well to chat room services, so we monitored 100 (a limitation of EFnet) of the most active EFnet-based IRC rooms, for over a month dating from November 26th through December 30th 2012. We monitored each of the 100 IRC channels for member joins, leaves, nickname changes, and messages. Anecdotally, we found that users often temporarily disconnect without

1 0.8 Members 0.6 0.4 0.2 0 0 0.1 Percent Days Missed 1 all data sets football iphone redhat stocks

1 0.8 Ratio 0.6 0.4 0.2 0 0.001 0.01 0.1 1 IRC Chatrooms Average Possinmity Worst-case Possinmity Correctly Guessed

Figure 4: The number of users missing no less than the specified ratio of days in the data set
1 0.8 Percent of Posts 0.6 0.4 0.2 0 0 0.2 0.4 0.6 0.8 1 Percent Days Missed all data sets football iphone redhat stocks

Figure 6: Attack analysis on the 100 EFnet IRC rooms eligibility 0.01 0.25 0.05 1.00 full trace client messages 217 113815 312 164304 355 175714 1234 196505 short trace client messages 310 7389 334 8991 363 9891 409 10157

Table 1: Activity for different eligibilities sorting metric. To focus on behavior within a single environment, we make use of only the football data set due to the average behavior of members online times but high member activity. The first row of Figure 7 shows these results, with the specific number of members and messages in Table 1 under "full trace." In these results, the importance of eligibility stands out as a significant factor. While all levels of eligibility show usability at buddy sets of 2 or more, the probability of guessing correctly only diminishes to a few percent at sizes of 16 to 32. At this point, the only usable eligibility levels include the members who miss less than 25% of the total days. When considering the whole set of users, we can see that ephemeral users make using this system impractical if we attempt to maintain long-lived pseudonyms. By comparing the lazy to the deity approach, we can see that using historical information as the basis for making future decisions seems promising, but clearly we can still make better decisions. With respect to the buddy sets of interest, average time delays average at 1 day, which matches the round intervals.

Figure 5: The number of posts associated with the members who missed the specified ratio of days. Correlates with the members shown in Figure 4. of members and messages included in different notions of eligibility. The results establish a correlation between the members with fewer missed intervals and significant portion of the total messages published. In football, for example, more than 80% of its messages are associated with those who miss fewer than 20% of the days in the data set, also representing 20% of the total number of users. Even the average case shows that around 40% of messages come from members who have not missed a day, again roughly 20% of the total membership.

5.3 Anonymity of Users without Buddies
To explore the limitations of possibilistic anonymity metrics, we employed the Anonymity Scheduler to behave as an adversary considering both possinimty and probabilitistic anonymity, presenting the results in Figure 6. Using possinimty alone, most members might feel secure. However, by analyzing the ability of the adversary to successfully guess the owner of a pseudonym given the probabilitic approach in Section 2.3.2, the adversary can correctly guess the ownership of a pseudonym around 20% of the time.

5.5 Buddy Sets with Possinymity
This evaluation focuses on the notion of possinimty as a tradeoff for obtaining better anonymity without having to resort to the rigidity of buddy sets. We examined many possinimity set sizes for various eligibilities for two buddy set sizes: 4 and 8, because of the decent balance between delivered messages and probability of correctly guessing, as shown in the first row of Figure 7. Our results, the second row of Figure 7, show that changes in possinimty effect the ability to deliver messages and their delays far less than increases in buddy set size. Of course, the trade-ff is that possinimty has a weaker effect on probability of correctly guessing than changes in buddy set size, but the results are not negligible.

5.4 Utility of Buddy Sets
We now investigate the impact varying buddy set sizes has on anonymity and utility, as defined by the amount of cleartext messages published and the delay induced. For brevity, we focus only on a deity policy, to show us what a potential upper bound might be and compare that to our lazy approach to forming buddy sets using offline times as the

5.6 Effect on Short Lived Sessions

Percent of delivered messages

0.8 Delivery delay 0.6 0.4 0.2 0 2 8 32 Buddy set size Deity at 0.01 Deity at 0.25 128

1day

Prob of guessing nym owner 2

1

1week

1

0.1

1hr

0.01

1min 0

0.001 2 8 32 Buddy set size Lazy at 0.50 Lazy at 1.00 128

Percent of delivered messages

0.8 Delivery delay 0.6 0.4 0.2 0 64 128 192 256 Possinimity set size 0.01 with 4 B 0.25 with 4 B

1day

Prob of guessing nym owner

1

8 32 128 Buddy set size Deity at 0.50 Lazy at 0.01 Deity at 1.00 Lazy at 0.25 1week

1

0.1

1hr

0.01

1min 0 64 128 192 256

0.001 64 128 192 256 Possinimity set size 0.50 with 8 B 1.00 with 8 B

Possinimity set size 0.50 with 4 B 0.01 with 8 B 1.00 with 4 B 0.25 with 8 B

Figure 7: Results from over a month of the IRC EFnet channel football In contrast to the evaluation in Section 5.4 that focused on the entire data set, we investigate the utility of Buddies during a significantly shorter period of time, for example, a lively discussion during an intense Monday Night Football game. Raucous members of the IRC room football exhibit significant aggression towards fans of the opposite team, to protect members from harming each other, they could use an anonymity system protected by Buddies to trash talk each other. In this evaluation, we construct a short-lived anonymity system using a 6 hour window during one of the busier times in the IRC football room trace. Unlike the longer trace, we used round intervals of 2 seconds and measured eligibility as members who miss fewer than some percent of these 2 second intervals. Our results are presented in Figure 8 with the specific number of members and messages in Table 1 under the header short trace. The graphs correlate well with the long lived evaluation, but in contrast to the full data set, a significantly greater ratio of members maintain eligibility. The time delays also exhibit interesting behavior. In this environment members demand interactive sessions, and while a strict eligibility metric maintains this all the remaining eligibilities exhibit undesirable delay after buddy sets of 16. Fortunately that correlates to roughly a few percent of the probability of correctly guessing a pseudonym. while Dissent incurred only 41 lines of additional C++ code. Included within the 528 lines of C++ code, Buddies comes with both a static and dynamic policy each weighing in at 89 and 172 lines of code, respectively. In Dissent, Buddies' Buddy Set concept applied cleanly; however, in the current Dissent implementation, maintaining possinimty required some compromises. Currently, Dissent servers perform the cleartext revealing process for all scheduled pseudonyms in parallel and while revealing each pseudonym iteratively would enable better use of possinimty metrics performing the operation iteratively would require adding additional interaction among the servers. The overhead of this process would only be negligible for rounds with interval time t in which there were m scheduled pseudonyms with inter-server network latency of l, where m	l  t. Fortunately, the Dissent implementation schedules pseudonyms to be available in different rounds and therefore Buddies can make a reactive decision with the assumption that every one of the scheduled pseudonyms may reveal cleartext. Using our Dissent implementation of Buddies, we focused evaluations on the additional delay added by Buddies. We constructed a network of 8 server machines, 64 client machines, and from 8 to 512 clients (running up to 8 clients on each client machine). Clients have a 50 millisecond delay and aggregate 100 M/bit connection to servers, while servers have a 10 millsecond delay to other servers but each has a dedicated 100 M/bit connection to every other server. To focus on the overheads Buddies incurs, we avoided sending cleartext messages across Dissent, The results (withheld due to space considerations) clearly show that Buddies imposes negligible overhead over Dissent, which is likely dominated by the expensive asymmetric cryptography used in signing and verifying messages.

5.7 Practical Considerations
The implementation of Buddies into Dissent sheds light into both the overheads of Buddies in a real system and the implementation complexities Buddies induces. We built both Buddies and a web service for querying the various Buddies meters and we modified Dissent [32] to 1) support the reactive and proactive analysis in the anonymity protocol and 2) transmit the set of online members (those with ciphertexts used in Dissent) along with the cleartext messages at the end of each round. Buddies totalled 528 lines of C++ code,

6. CONCLUSION

Percent of delivered messages

1400 1200 1000 800 600 400 200 0 1 2

0.8 0.6 0.4 0.2 0 1 2 4 8 16 32 64 128 Buddy set size deity-0.01 deity-0.25

Prob of guessing nym owner 4 8 16 32 64 128 Buddy set size dynamic-0.01 dynamic-0.25

1 Delivery delay (seconds)

1600

1

0.1

0.01

0.001 1 4 8 16 32 64 128 Buddy set size dynamic-0.5 dynamic-1.0 2

deity-0.5 deity-1.0

Figure 8: Results from a 6 hour window during a Monday Night Football game Buddies offers the first systematic architecture addressing long-term intersection attacks in anonymity systems, by offering passive metrics of vulnerabiity and active control policies. While only a first step leaving many open questions, our trace-based simulations and working prototype suggest that Buddies may point to practical ways of further protecting anonymity-sensitive users of online forums. [13] A. Haeberlen, P. Kouznetsov, and P. Druschel. PeerReview: Practical accountability for distributed systems. In 21st SOSP, Oct. 2007. [14] N. Hopper and E. Y. Vasserman. On the effectiveness of k-anonymity against traffic analysis and surveillance. In WPES, Oct. 2006. [15] D. Kedogan, D. Agrawal, and S. Penz. Limits of anonymity in open environments. In 5th International Workshop on Information Hiding, Oct. 2002. [16] D. Kelly, R. Raines, R. Baldwin, B. Mullins, and M. Grimaila. Towards mathematically modeling the anonymity reasoning ability of an adversary. In IPCCC, pages 524531. IEEE, 2008. [17] D. J. Kelly. A taxonomy for and analysis of anonymous communications networks. PhD thesis, Wright Patterson AFB, OH, USA, 2009. AAI3351544. [18] B. Levine, M. Reiter, C. Wang, and M. Wright. Timing attacks in low-latency mix systems. In A. Juels, editor, Financial Cryptography, volume 3110 of Lecture Notes in Computer Science, pages 251265. Springer Berlin / Heidelberg, 2004. [19] J. K. Liu, V. K. Wei, and D. S. Wong. Linkable spontaneous anonymous group signature for ad hoc groups. In Australian Conference on Information Security and Privacy, July 2004. [20] U. Moeller and L. Cottrell. Mixmaster protocol: Version 2, Jan. 2000. http://www.eskimo.com/ ~rowdenw/crypt/Mix/ draft-moeller-mixmaster2-protocol-00.txt. [21] S. J. Murdoch and G. Danezis. Low-cost traffic analysis of tor. In Security and Privacy, 2005 IEEE Symposium on. IEEE, 2005. [22] C. A. Neff. A verifiable secret shuffle and its application to e-voting. In CCS, Nov. 2001. [23] A. Pfitzmann and M. Hansen. A terminology for talking about privacy by data minimization: Anonymity, unlinkability, undetectability, unobservability, pseudonymity, and identity management, Aug. 2010. [24] A. Pfitzmann, B. Pfitzmann, and M. Waidner. ISDN-mixes: Untraceable communication with very small bandwidth overhead. In GI/ITG Conference on Communication in Distributed Systems, February 1991. [25] J.-F. Raymond. Traffic analysis: Protocols, attacks, design issues and open problems. In Workshop on

Acknowledgments
This material is based upon work supported by the Defense Advanced Research Agency (DARPA) and SPAWAR Systems Center Pacific, Contract No. N66001- 11-C-4018.

7.

REFERENCES

[1] O. Berthold, A. Pfitzmann, and R. Standtke. The disadvantages of free MIX routes and how to overcome them. In Workshop on Design Issues in Anonymity and Unobservability, pages 3045, July 2000. [2] D. Chaum. Untraceable electronic mail, return addresses, and digital pseudonyms. Communications of the ACM, Feb. 1981. [3] D. Chaum. The dining cryptographers problem: Unconditional sender and recipient untraceability. Journal of Cryptology, Jan. 1988. [4] H. Corrigan-Gibbs and B. Ford. Dissent: accountable anonymous group messaging. In CCS, Oct. 2010. [5] H. Corrigan-Gibbs, D. I. Wolinsky, and B. Ford. Proactively accountable anonymous messaging in verdict. In USENIX Security, Aug. 2013. [6] G. Danezis and A. Serjantov. Statistical disclosure or intersection attacks on anonymity systems. In Information Hiding, May 2004. [7] C. D iaz, S. Seys, J. Claessens, and B. Preneel. Towards measuring anonymity. In Proceedings of the 2nd international conference on Privacy enhancing technologies, PET'02, 2003. [8] R. Dingledine, N. Mathewson, and P. Syverson. Tor: the second-generation onion router. In USENIX Security Symposium, 2004. [9] J. R. Douceur. The Sybil attack. In 1st International Workshop on Peer-to-Peer Systems, Mar. 2002. [10] E. Fujisaki and K. Suzuki. Traceable ring signature. In 10th PKC, Apr. 2007. [11] J. Furukawa and K. Sako. An efficient scheme for proving a shuffle. In CRYPTO, Aug. 2001. [12] P. Golle and A. Juels. Dining cryptographers revisited. Eurocrypt, May 2004.

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]

Design Issues in Anonymity and Unobservability, pages 1029, 2000. A. Serjantov and G. Danezis. Towards an information theoretic metric for anonymity. In Proceedings of the 2nd international conference on Privacy enhancing technologies, PET'02, 2003. V. Shmatikov and M.-H. Wang. Measuring relationship anonymity in mix networks. In WPES, Oct. 2006. V. Shmatikov and M.-H. Wang. Timing analysis in low-latency mix networks: Attacks and defenses. In Computer SecurityESORICS 2006. Springer, 2006. E. G. Sirer, S. Goel, M. Robson, and D. Engin. Eluding carnivores: File sharing with strong anonymity. In SIGOPS EW, Sept. 2004. N. Tran, B. Min, J. Li, and L. Submaranian. Sybil-resilient online content voting. In 6th NSDI, Apr. 2009. M. Waidner. Unconditional sender and recipient untraceability in spite of active attacks. In Eurocrypt, pages 302319, Apr. 1989. D. I. Wolinsky, H. Corrigan-Gibbs, A. Johnson, and B. Ford. Dissent in numbers: Making strong anonymity scale. In 10th OSDI, Oct. 2012. H. Yu, P. B. Gibbons, M. Kaminsky, and F. Xiao. SybilLimit: A near-optimal social network defense against sybil attacks. In IEEE Symposium on Security and Privacy, May 2008.

first place, and the attacker must adjust their reliability profile after learning "enough" about N 's owner, but before too many buddy set splits have already occurred for N . Thus, if the Policy Oracle builds up user reputation information in a relatively conservative, long-term fashion across the users' entire histories (e.g., from before N appeared), this may make it difficult for an attacker to "steer" malicious users' reliability profiles "late in the game" to implement a cluster attack N . Clustering attacks nevertheless present a risk that more randomized buddy set formation policies may reduce. As in any distributed system, an attacker may be able to amplify the effective numbers of malicious clients via Sybil attacks [9], creating many fake user identities. Buddies addresses this risk by requiring users to be authenticated-- via linkable ring signatures as detailed above--as owners of "real" identities in some Sybil attack resistant identity space. Buddies is agnostic as to the exact nature of this public identity space or how it is made resistant to Sybil attacks. The current prototype simply is defined for "closed" groups, defined by a static roster of public keys listing all members, so the group is exactly as Sybil attack resistant as whatever method the group's creater uses to form the roster. To support open-ended groups, Buddies could build on one of the many Sybil attack resistance schemes, such as those based on social networks [30, 33]--or could simply rate-limit Sybil attacks via some "barrier to entry," e.g., requiring users to solve a CAPTCHA or receive a phone callback to "register" an unknown public key for participation.

APPENDIX A. MALICIOUS USERS AND SYBIL ATTACKS
While Buddies can measure, and optionally enforce a lower bound on, the number of users comprising a Nym's possinymity or indinymity set, Buddies cannot guarantee that all those users are providing useful anonymity. In particular, if the owner of a Nym N has specified a policy mandating a minimum buddy-set size of K , but up to F other clients may be colluding with the adversary, then N 's owner may have to assume that its actual minimum anonymity set size may be as little as K - F , if all F bad clients happen to--or somehow arrange to--land in the same buddy set as N 's owner. Since in practical systems we don't expect users to have a reliable way of "knowing" how many other clients are conspiring against them, we treat F as an unknown variable that users may simply have to "guess" and factor into their choices of possinymity or indinymity lower-bounds. In this respect Buddies is no different from any other anonymity system some of whose users may be compromised. Reducing vulnerability to malicious clients may be an argument in favor of random buddy-set formation (Section 3.2). Randomized policies may offer some guarantee that the malicious users present in a Nym's initial user set become "evenly distributed"among buddy-sets. In any preferential, "reputationbased" formation scheme, if the attacker can learn or correctly guess the general "level of reliability" of a Nym N 's true owner--which may well be inferrable from N 's posting record--then the attacker's compromised nodes might deliberately exhibit a similar level of reliability in hopes of getting clustered together in the owner's buddy set. For such attacks to succeed, however, the malicious users must be present at N 's creation in order to fall in N 's possinymity set in the

